""" AST Extractor module """

import os
from collections import defaultdict
from datetime import datetime
import pandas as pd
import git
import whatthepatch
from vulnerability_prediction.ast_extraction.parser import ClangParser
from vulnerability_prediction import REPOS
from vulnerability_prediction.utils import LOGGER


class ASTExtractor():
    """ AST Extractor class for a single repo """

    def __init__(self, repo_key, repo_def):
        self.repo_key = repo_key
        self.repo_def = repo_def
        self.repo = git.Repo(repo_def["path"])
        self.output_dir = repo_def["output_dir"]
        self.ast_output_dir = repo_def["ast_output_dir"]
        self.parser = ClangParser()

    def __get_bug_inducing_df(self):
        """Reads bug inducing file changes dataset"""

        file_name = "{}_bug_inducing_file_changes.csv".format(self.repo_key)
        bug_inducing_path = os.path.join(self.output_dir, file_name)

        return pd.read_csv(bug_inducing_path).dropna()

    def __get_file_changes_df(self):
        """Reads file changes dataset"""

        file_name = "{}_file_changes.csv".format(self.repo_key)
        file_changes_path = os.path.join(self.output_dir, file_name)
        return pd.read_csv(file_changes_path)

    def __get_summary_df(self):
        """ Reads summary file into a pd.DataFrame """
        summary_path = os.path.join(self.ast_output_dir, "summary.csv")
        if os.path.exists(summary_path):
            return pd.read_csv(summary_path, index_col=[0])

        return pd.DataFrame(
            columns=["commit_id", "file", "commit_date", "func_name",
                     "out_file", "is_fixing", "is_vuln", "extent_start",
                     "extent_end", "export_date"])

    def __create_output_folders(self):
        """Create output folders if they don't exists."""

        if not os.path.isdir(os.path.join(self.ast_output_dir, "vuln")):
            os.makedirs(os.path.join(self.ast_output_dir, "vuln"))

        if not os.path.isdir(os.path.join(self.ast_output_dir, "fixing")):
            os.makedirs(os.path.join(self.ast_output_dir, "fixing"))

        if not os.path.isdir(os.path.join(self.ast_output_dir, "notvuln")):
            os.makedirs(os.path.join(self.ast_output_dir, "notvuln"))

    @staticmethod
    def get_changed_lines(patch):
        """
        Get changed lines in a patch
        l[0]: old a.k.a. deleted
        l[1]: new a.k.a. added
        """
        changes = [c for c in patch.changes if c is not None]
        changed_lines = [l for l in changes if l[0]
                         is None or l[1] is None or l[0] != l[1]]

        # remove comment lines and trim the strings
        changed_lines = list(
            map(lambda x: (x[0], x[1], x[2].strip()), changed_lines))
        changed_lines = [c for c in changed_lines if not (
            c[2].startswith('//') or c[2].startswith('/*'))]

        deleted_lines = [l[0] for l in changed_lines if l[0]
                         is not None]  # if l[1] is None
        deleted_tokens = [l[2] for l in changed_lines if l[0] is not None]

        added_lines = [l[1] for l in changed_lines if l[0] is None
                       and l[1] is not None]
        added_tokens = [l[2] for l in changed_lines
                        if l[0] is None and l[1] is not None]

        if len(deleted_lines) == 0:
            deleted_lines = added_lines.copy()
            deleted_tokens = added_tokens.copy()
            # [l-1 for l in added_lines]

        return deleted_lines, deleted_tokens

    def _extract_ast(self, row, patch, summary_df, file_versions):
        """
        Extract AST of a particular commit-file pair.
        """
        summary = []
        if patch.changes is None:
            return None

        file = row.file  # patch.header.new_path
        file_path = os.path.join(self.repo.working_dir, file)

        changed_lines, _ = self.get_changed_lines(patch)

        self.repo.git.checkout(row.commit_id)
        nodes = self.parser.parse(self.repo.working_dir, file)
        if nodes is None:
            return None

        all_funcs = self.parser.find_functions(nodes)

        if row.is_vuln == 1:
            cond = ((summary_df.commit_id == row.commit_id_fixing) &
                    (summary_df.file == row.file) &
                    (summary_df.is_fixing == 1))

            fixing_df = summary_df[cond]
            func_names = fixing_df.func_name.drop_duplicates().values

            vuln_funcs = [f for f in all_funcs if f.spelling in func_names]
            all_funcs = [f for f in all_funcs if f not in vuln_funcs]

            LOGGER.info("%s, %s, %d func names, %d func nodes",
                        row.commit_id, row.file, len(func_names),
                        len(vuln_funcs))

            summary_vuln = self._save_functions(
                vuln_funcs, nodes, row, file_path, "vuln", 0, 1,
                file_versions)
            summary.extend(summary_vuln)

        if row.is_fixing == 1:
            changed_funcs = self.parser.find_parent_function(
                nodes, changed_lines)
            msg = "{} fixing nodes for {}-{}".format(len(changed_funcs),
                                                     row.commit_id, file)
            LOGGER.info(msg)

            all_funcs = [f for f in all_funcs if f not in changed_funcs]

            summary_fixing = self._save_functions(
                changed_funcs, nodes, row, file_path, "fixing", 1, 0,
                file_versions)
            summary.extend(summary_fixing)

        # export the rest
        summary_rest = self._save_functions(
            all_funcs, nodes, row, file_path, "notvuln", 0, 0, file_versions)
        summary.extend(summary_rest)

        return summary

    def _has_duplicate(self, node_list, current_versions):
        return any([self.parser.compare_trees(node_list, cur)
                    for cur in current_versions])

    def _save_functions(self, funcs, nodes, row, full_name, func_type,
                        is_fixing, is_vuln, file_versions,
                        skip_duplicates=True):
        """Saves AST of functions in the current revision of the file"""
        summary = []
        for n in funcs:
            if n.file != full_name:
                continue

            file_name = row.file.replace("/", "_").replace("//", "_")
            func_name = n.spelling

            current_versions = [v["nodes"]
                                for v in file_versions[row.file][func_name]]
            node_list, _ = self.parser.get_nodelist(nodes, [n])

            if skip_duplicates and self._has_duplicate(node_list,
                                                       current_versions):
                LOGGER.info("%s matches an earlier version", func_name)
                continue

            file_versions[row.file][func_name].append(
                {"commit_id": row.commit_id, "nodes": node_list})

            file_name = "---".join([file_name, func_name,
                                    func_type, row.commit_id])

            output_file = os.path.join(self.ast_output_dir, func_type,
                                       file_name)

            self.parser.export_tree(node_list, output_file)

            summary.append({
                "commit_id": row.commit_id,
                "file": row.file,
                "commit_date": row.commit_date,
                "func_name": func_name,
                "out_file": output_file,
                "is_fixing": is_fixing,
                "is_vuln": is_vuln,
                "extent_start": n.extent_start,
                "extent_end": n.extent_end,
                "export_date": str(datetime.now())
            })

        LOGGER.info("Saved %d %s funcs", len(summary), func_type)
        return summary

    @staticmethod
    def merge_datasets(inducing_df, file_changes_df, related_only=True):
        """
        Merge inducing_df and file_changes_df and
        label vulnerability inducing file changes
        :param: inducing_df: pd.DataFrame
            DataFrame of incuding_commit, file, fixing_commit tuples
        :param: file_changes_df: pd.DataFrame
            DataFrame of commit-file pairs
        :param: related_only: bool
            If True, keep only the files which have a vulnerability record
            in its' history
        :returns: pd.DataFrame
            DataFrame of file-change information
            with is_vuln and is_fixing labels
        """

        if related_only:
            files = inducing_df.file.drop_duplicates().values
            file_changes_df = file_changes_df[file_changes_df.file.isin(files)]

        # keep only relevant source code files.

        allowed_exts = ["c", "cc", "cpp", "h", "py", "js"]
        file_changes_df = file_changes_df[
            file_changes_df.file.str.split(".").str[-1].isin(allowed_exts)]

        merged = file_changes_df.merge(inducing_df,
                                       left_on=["commit_id", "file"],
                                       right_on=["inducing_id", "file"],
                                       how="left")

        merged_ = merged.merge(file_changes_df,
                               left_on=["fixing_id", "file"],
                               right_on=["commit_id", "file"],
                               how="left",
                               suffixes=('', '_fixing'))

        merged_["is_vuln"] = 0
        merged_["is_fixing"] = 0

        fixing_ids = [tuple(v) for v in inducing_df[[
            "fixing_id", "file"]].dropna().values]

        merged_.loc[merged_.inducing_id.notnull(), "is_vuln"] = 1
        merged_.loc[merged_.apply(lambda r: (
            r["commit_id"], r["file"]) in fixing_ids, axis=1), "is_fixing"] = 1

        for _, row in merged_[merged_.commit_date_fixing.notnull()].iterrows():
            cols = ["is_vuln", "commit_id_fixing", "commit_date_fixing"]
            cond = ((merged_.file == row.file) &
                    (merged_.commit_date >= row.commit_date) &
                    (merged_.commit_date < row.commit_date_fixing))
            merged_.loc[cond, cols] = [
                1, row.commit_id_fixing, row.commit_date_fixing]

        return merged_.sort_values(["file", "commit_date"], ascending=False)

    @staticmethod
    def get_reamining(file_change_dataset, summary_df):
        """Returns file changes which have not been extracted"""
        file_change_dataset = (
            pd.merge(file_change_dataset, summary_df[["commit_id", "file"]],
                     how="left", on=["commit_id", "file"], indicator=True,
                     suffixes=('', '_'))
            .query("_merge == 'left_only'")[file_change_dataset.columns]
        )

        return file_change_dataset

    def ast_extraction(self, file_change_dataset, summary_df):
        """
        Iterates over commit-file pairs and extracts AST
        for each function for each revision.
        Uses commit patches to find vulnerability inducing methods
        """
        summary_path = os.path.join(self.ast_output_dir, "summary.csv")

        self.repo.git.checkout("master")

        last_commit = ''
        last_patch = None

        file_versions = defaultdict(lambda: defaultdict(list))

        i = 0
        for _, row in file_change_dataset.iterrows():
            try:
                i += 1
                msg = "[{}/{}] {}, {}, {}".format(i, len(file_change_dataset),
                                                  row.commit_id, row.file,
                                                  row.commit_date)
                LOGGER.info(msg)
                if row.commit_id != last_commit:
                    diff = self.repo.git.diff(
                        row.commit_id + '~', row.commit_id)
                    patch = list(whatthepatch.parse_patch(diff))
                    last_patch = patch.copy()
                    last_commit = row.commit_id
                else:
                    patch = last_patch.copy()

                file_patch = [
                    p for p in patch if p.header.new_path == row.file][0]
                summ = self._extract_ast(row, file_patch, summary_df,
                                         file_versions)
                summary_df = pd.concat(
                    [summary_df, pd.DataFrame(summ)], axis=0)
                summary_df.to_csv(summary_path)
            except Exception as ex:
                LOGGER.error(ex, exc_info=True)

    def run(self):
        """ Manage ast extraction process """
        LOGGER.info("Extracting AST's for %s...", self.repo_key)

        inducing_df = self.__get_bug_inducing_df()
        file_changes_df = self.__get_file_changes_df()

        summary_df = self.__get_summary_df()

        file_change_dataset = self.merge_datasets(inducing_df, file_changes_df)
        file_change_dataset = self.get_reamining(
            file_change_dataset, summary_df)

        self.__create_output_folders()

        self.ast_extraction(file_change_dataset, summary_df)


def run():
    """Main loop of AST extraction process"""
    for repo_entry in REPOS.items():
        repo_key, repo_def = repo_entry
        ast_extractor = ASTExtractor(repo_key, repo_def)
        ast_extractor.run()


if __name__ == "__main__":
    run()
