""" AST Extractor module """

import os
import traceback
from collections import defaultdict
import pandas as pd
import git
import whatthepatch
from vulnerability_prediction.ast_extraction.parser import ClangParser
from vulnerability_prediction import REPOS

PARSER = ClangParser()


def get_changed_lines(patch):
    """
    Get changed lines in a patch
    l[0]: old a.k.a. deleted
    l[1]: new a.k.a. added
    """
    changes = [c for c in patch.changes if c is not None]
    changed_lines = [l for l in changes if l[0]
                     is None or l[1] is None or l[0] != l[1]]

    # remove comment lines and trim the strings
    changed_lines = list(
        map(lambda x: (x[0], x[1], x[2].strip()), changed_lines))
    changed_lines = [c for c in changed_lines if not (
        c[2].startswith('//') or c[2].startswith('/*'))]

    deleted_lines = [l[0] for l in changed_lines if l[0]
                     is not None]  # if l[1] is None
    deleted_tokens = [l[2] for l in changed_lines if l[0] is not None]

    added_lines = [l[1] for l in changed_lines if l[0] is None
                   and l[1] is not None]
    added_tokens = [l[2] for l in changed_lines
                    if l[0] is None and l[1] is not None]

    if len(deleted_lines) == 0:
        deleted_lines = added_lines.copy()
        deleted_tokens = added_tokens.copy()
        # [l-1 for l in added_lines]

    return deleted_lines, deleted_tokens


def extract_ast(row, repo, patch, summary_df, file_versions, ast_output_dir):
    """
    Extract AST of a particular commit-file pair.
    """
    summary = []
    if patch.changes is None:
        return None

    file = row.file  # patch.header.new_path
    file_path = os.path.join(repo.working_dir, file)

    changed_lines, _ = get_changed_lines(patch)

    repo.git.checkout(row.commit_id)
    nodes = PARSER.parse(repo.working_dir, file)
    if nodes is None:
        return None

    all_funcs = PARSER.find_functions(nodes)

    if row.is_vuln == 1:
        fixing_df = summary_df[(summary_df.commit_id == row.commit_id_fixing) &
                               (summary_df.file == row.file) &
                               (summary_df.is_fixing == 1)]

        func_names = fixing_df.func_name.drop_duplicates().values

        vuln_funcs = [f for f in all_funcs if f.spelling in func_names]
        all_funcs = [f for f in all_funcs if f not in vuln_funcs]

        print("{}, {}, {} func names, {} func nodes".format(
            row.commit_id, row.file, len(func_names), len(vuln_funcs)))

        summary_vuln = save_functions(vuln_funcs, nodes, row, file_path,
                                      "vuln", 0, 1, file_versions,
                                      ast_output_dir)
        summary.extend(summary_vuln)

    if row.is_fixing == 1:
        changed_funcs = PARSER.find_parent_function(
            nodes, changed_lines)
        print("{} fixing nodes for {}-{}".format(len(changed_funcs),
                                                 row.commit_id, file))

        all_funcs = [f for f in all_funcs if f not in changed_funcs]
        summary_fixing = save_functions(changed_funcs, nodes, row, file_path,
                                        "fixing", 1, 0, file_versions,
                                        ast_output_dir)
        summary.extend(summary_fixing)

    # export the rest
    summary_rest = save_functions(all_funcs, nodes, row, file_path, "notvuln",
                                  0, 0, file_versions, ast_output_dir)
    summary.extend(summary_rest)

    return summary


def save_functions(funcs, nodes, row, full_name, func_type, is_fixing,
                   is_vuln, file_versions, ast_output_dir,
                   skip_duplicates=True):
    """Saves AST of functions in the current revision of the file"""
    summary = []
    for n in funcs:
        if n.file != full_name:
            continue

        file_name = row.file.replace("/", "_").replace("//", "_")
        func_name = n.spelling

        current_versions = [v["nodes"]
                            for v in file_versions[row.file][func_name]]
        node_list, _ = PARSER.get_nodelist(nodes, [n])

        if skip_duplicates and any([PARSER.compare_trees(node_list, cur)
                                    for cur in current_versions]):
            print("{} matches an earlier version".format(func_name))
            continue

        file_versions[row.file][func_name].append(
            {"commit_id": row.commit_id, "nodes": node_list})

        file_name = "---".join([file_name, func_name,
                                func_type, row.commit_id])

        output_file = os.path.join(ast_output_dir, func_type, file_name)
        PARSER.export_tree(node_list, output_file)

        summary.append({
            "commit_id": row.commit_id,
            "file": row.file,
            "commit_date": row.commit_date,
            "func_name": func_name,
            "out_file": output_file,
            "is_fixing": is_fixing,
            "is_vuln": is_vuln,
            "extent_start": n.extent_start,
            "extent_end": n.extent_end
        })

    print("Saved {} {} funcs".format(len(summary), func_type))
    return summary


def merge_datasets(inducing_df, file_changes_df, related_only=True):
    """
    Merge inducing_df and file_changes_df and
    label vulnerability inducing file changes
    :param: inducing_df: pd.DataFrame
        DataFrame of incuding_commit, file, fixing_commit tuples
    :param: file_changes_df: pd.DataFrame
        DataFrame of commit-file pairs
    :param: related_only: bool
        If True, keep only the files which have a vulnerability record
        in its' history
    :returns: pd.DataFrame
        DataFrame of file-change information
        with is_vuln and is_fixing labels
    """
    if related_only:
        files = inducing_df.file.drop_duplicates().values
        file_changes_df = file_changes_df[file_changes_df.file.isin(files)]

    # keep only relevant source code files.

    allowed_extensions = ["c", "cc", "cpp", "h", "py", "js"]
    file_changes_df = file_changes_df[
        file_changes_df.file.str.split(".").str[-1].isin(allowed_extensions)]

    merged = file_changes_df.merge(inducing_df,
                                   left_on=["commit_id", "file"],
                                   right_on=["inducing_id", "file"],
                                   how="left")

    merged_ = merged.merge(file_changes_df,
                           left_on=["fixing_id", "file"],
                           right_on=["commit_id", "file"],
                           how="left",
                           suffixes=('', '_fixing'))

    merged_["is_vuln"] = 0
    merged_["is_fixing"] = 0

    fixing_ids = [tuple(v)
                  for v in inducing_df[["fixing_id", "file"]].dropna().values]

    merged_.loc[merged_.inducing_id.notnull(), "is_vuln"] = 1
    merged_.loc[merged_.apply(lambda r: (
        r["commit_id"], r["file"]) in fixing_ids, axis=1), "is_fixing"] = 1

    for _, row in merged_[merged_.commit_date_fixing.notnull()].iterrows():
        cols = ["is_vuln", "commit_id_fixing", "commit_date_fixing"]
        merged_.loc[(merged_.file == row.file) &
                    (merged_.commit_date >= row.commit_date) &
                    (merged_.commit_date < row.commit_date_fixing),
                    cols] = [1, row.commit_id_fixing, row.commit_date_fixing]

    return merged_.sort_values(["file", "commit_date"], ascending=False)


def get_bug_inducing_df(repo_key, output_dir):
    """Reads bug inducing file changes dataset"""
    bug_inducing_path = os.path.join(
        output_dir, "{}_bug_inducing_file_changes.csv".format(repo_key))

    inducing_df = pd.read_csv(bug_inducing_path)
    return inducing_df.dropna()


def get_file_changes_df(repo_key, output_dir):
    """Reads file changes dataset"""
    file_changes_path = os.path.join(
        output_dir, "{}_file_changes.csv".format(repo_key))
    return pd.read_csv(file_changes_path)


def get_reamining(file_change_dataset, summary_df):
    """Returns file changes which have not been extracted"""
    file_change_dataset = (
        pd.merge(file_change_dataset, summary_df[["commit_id", "file"]],
                 how="left", on=["commit_id", "file"], indicator=True,
                 suffixes=('', '_'))
        .query("_merge == 'left_only'")[file_change_dataset.columns]
    )

    return file_change_dataset


def create_output_folders(ast_output_dir):
    """Create output folders if they don't exists."""

    if not os.path.isdir(os.path.join(ast_output_dir, "vuln")):
        os.makedirs(os.path.join(ast_output_dir, "vuln"))

    if not os.path.isdir(os.path.join(ast_output_dir, "fixing")):
        os.makedirs(os.path.join(ast_output_dir, "fixing"))

    if not os.path.isdir(os.path.join(ast_output_dir, "notvuln")):
        os.makedirs(os.path.join(ast_output_dir, "notvuln"))


def ast_extraction(repo, file_change_dataset, summary_df,
                   summary_path, ast_output_dir):
    """
    Iterates over commit-file pairs and extracts AST
    for each function for each revision.
    Uses commit patches to find vulnerability inducing methods
    """
    repo.git.checkout("master")

    last_commit = ''
    last_patch = None

    file_versions = defaultdict(lambda: defaultdict(list))

    i = 0
    for _, row in file_change_dataset.iterrows():
        try:
            i += 1
            print("[{}/{}] {}, {}, {}".format(i, len(file_change_dataset),
                                              row.commit_id, row.file,
                                              row.commit_date))
            if row.commit_id != last_commit:
                diff = repo.git.diff(row.commit_id + '~', row.commit_id)
                patch = list(whatthepatch.parse_patch(diff))
                last_patch = patch.copy()
                last_commit = row.commit_id
            else:
                patch = last_patch.copy()

            file_patch = [p for p in patch if p.header.new_path == row.file][0]
            summ = extract_ast(row, repo, file_patch, summary_df,
                               file_versions, ast_output_dir)
            summary_df = pd.concat([summary_df, pd.DataFrame(summ)], axis=0)
            summary_df.to_csv(summary_path)
        except Exception as ex:
            print(ex)
            print(traceback.format_exc())


def run():
    """Main loop of AST extraction process"""
    for repo_entry in REPOS.items():
        repo_key, repo_def = repo_entry

        print("Extracting AST's for {}...".format(repo_key))
        repo = git.Repo(repo_def["path"])

        inducing_df = get_bug_inducing_df(repo_key, repo_def["output_dir"])
        file_changes_df = get_file_changes_df(repo_key, repo_def["output_dir"])

        summary_path = os.path.join(repo_def["ast_output_dir"], "summary.csv")
        if os.path.exists(summary_path):
            summary_df = pd.read_csv(summary_path, index_col=[0])
        else:
            summary_df = pd.DataFrame(
                columns=["commit_id", "file", "commit_date", "func_name",
                         "out_file", "is_fixing", "is_vuln", "extent_start",
                         "extent_end"]
            )

        file_change_dataset = merge_datasets(inducing_df, file_changes_df)
        file_change_dataset = get_reamining(file_change_dataset, summary_df)

        create_output_folders(repo_def["ast_output_dir"])

        ast_extraction(repo, file_change_dataset, summary_df,
                       summary_path, repo_def["ast_output_dir"])


if __name__ == "__main__":
    run()
