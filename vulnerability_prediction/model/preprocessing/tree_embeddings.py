""" Build tree embeddings using AST node kinds """

import os
import pickle
import numpy as np
from tqdm import tqdm

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

from vulnerability_prediction import REPOS
from vulnerability_prediction.utils import LOGGER

AST_DIR = REPOS["wireshark"]["ast_output_dir"]


def get_trees():
    """ Read AST summaries """
    with open(os.path.join(AST_DIR, "trees.pkl"), "rb") as f:
        return pickle.load(f)


def get_node_kinds(trees):
    """ Get node kinds and their id mappings """

    all_kinds = sum([t["df"].kind.values.tolist() for t in trees], [])
    all_kinds = list(dict.fromkeys(all_kinds).keys())

    all_kinds.insert(0, "NIL")

    kind_ids = {kind: i for i, kind in enumerate(all_kinds)}
    kind_ids_rev = dict(enumerate(all_kinds))

    return kind_ids, kind_ids_rev


def get_tree_positional_input(tree, kind_ids):
    """ Build positional input for a given tree """

    df = tree["df"]
    input_data = []
    target = []

    for _, row in df.iterrows():
        kind = kind_ids[row.kind]

        children = df[df.parent_uuid == row.uuid].kind.values.tolist()
        parent = df[df.uuid == row.parent_uuid].kind.values.tolist()

        if len(parent) == 0:
            parent.append("NIL")

        parent = [kind_ids[k] for k in parent]
        children = [kind_ids[k] for k in children]

        input_data.append(parent + children)
        target.append(kind)

    return input_data, target


def get_tree_input(trees, kind_ids):
    """ Builds inputs and targets for all trees """
    input_data = []
    targets = []
    for tree in trees:
        relation_data, target = get_tree_positional_input(tree, kind_ids)
        input_data.extend(relation_data)
        targets.extend(target)

    # apply padding
    max_len = max([len(d) for d in input_data])
    for data_ in input_data:
        if len(data_) < max_len:
            data_.extend([0] * (max_len - len(data_)))

    return np.asarray(input_data), np.asarray(targets), max_len


class TreeDataLoader(Dataset):
    """Text data loader for batches"""

    def __init__(self, x, y):
        self.x = x
        self.y = y

    def __getitem__(self, i):
        return self.x[i], self.y[i]

    def __len__(self):
        return len(self.y)


class PositionalEmbedding(nn.Module):
    """
    PositionalEmbedding model.
    Builds embedding based on nodes' position on the tree
    """

    def __init__(self, vocab_size, max_len, embedding_dim=128):
        super(PositionalEmbedding, self).__init__()

        self.embeddings = nn.Embedding(vocab_size, embedding_dim)

        self.fc1 = nn.Linear(max_len * embedding_dim, 256)
        self.fc2 = nn.Linear(256, 128)

    def forward(self, inp):
        out = self.embeddings(inp[:, 1:])
        out = out.view(out.shape[0], 1, -1)

        out2 = self.embeddings(inp[:, 0])
        out2 = out2.view(out2.shape[0], 1, -1)

        out = torch.cat([out2, out], 2)

        out = F.relu(self.fc1(out))
        out = self.fc2(out)

        log_probs = F.log_softmax(out, dim=2)

        out = out.view(out.shape[0], -1)
        log_probs = log_probs.view(log_probs.shape[0], -1)

        return out, log_probs


def train(model, data_loader, kind_ids_rev, n_epoch=30):
    """Training process"""
    losses = []
    loss_function = nn.NLLLoss()

    optimizer = optim.Adam(model.parameters(), lr=1e-3)

    vectors = {}

    for epoch in range(n_epoch):
        print("epoch", epoch)
        total_loss = 0

        for i, data in tqdm(enumerate(data_loader)):
            inp, target = data
            inp = inp.cuda()
            target = target.cuda()
            target_ = target.detach().cpu().numpy()

            model.zero_grad()

            embedding_vec, log_probs = model(inp)
            for j, t in enumerate(target_):
                vectors[kind_ids_rev[t]] = embedding_vec.detach().cpu().numpy()[
                    j].flatten()

            loss = loss_function(log_probs, target)

            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            losses.append(total_loss)

        print("Epoch", epoch, " total loss", total_loss)

    return vectors, losses


def visualize(vectors, three_d=False):
    """
    Visualize vectors
    """
    n_components = 3 if three_d else 2

    pca = PCA(n_components=n_components)
    vecs = np.array(list(vectors.values()))
    keys = list(vectors.keys())

    transformed = pca.fit_transform(vecs)

    if three_d:
        fig = plt.figure(figsize=(32, 15))
        ax = fig.add_subplot(111, projection='3d')

        for i, key in enumerate(keys):
            x_ = transformed[i, 0]
            y_ = transformed[i, 1]
            z_ = transformed[i, 2]
            ax.scatter(x_, y_, z_, marker='x', color='red')
            ax.text(x_ + 0.005, y_ + 0.005, z_ + 0.005, key, fontsize=9)

        fig.show()
    else:
        fig = plt.figure(figsize=(32, 15))

        for i, key in enumerate(keys):
            x_ = transformed[i, 0]
            y_ = transformed[i, 1]
            plt.scatter(x_, y_, marker='x', color='red')
            plt.text(x_ + 0.005, y_ + 0.005, key, fontsize=9)

        fig.show()


def run():
    """ Main procedure """
    trees = get_trees()
    kind_ids, kind_ids_rev = get_node_kinds(trees)

    x, y, max_len = get_tree_input(trees, kind_ids)
    dataset = TreeDataLoader(x, y)
    data_loader = DataLoader(dataset, batch_size=32,
                             num_workers=0, shuffle=True)

    model = PositionalEmbedding(vocab_size=len(kind_ids), 
                                max_len=max_len).cuda()

    pos_vectors, _ = train(model, data_loader, kind_ids_rev, 50)

    visualize(pos_vectors, three_d=False)


if __name__ == "__main__":
    run()
