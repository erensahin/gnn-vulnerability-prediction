""" Scraper module for mozilla """

import re
import os
import urllib.parse
import requests
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
from vulnerability_prediction import REPOS
from vulnerability_prediction.scrapers import FILE_SUFFIX

BASE_URL = 'https://www.mozilla.org'
ADV_URL = 'https://www.mozilla.org/en-US/security/advisories/'
BUGZILLA_URL = 'bugzilla.mozilla.org'

GECKO_OUTPUT = REPOS["gecko-dev"]["output_dir"]
RELEASES_OUTPUT = REPOS["releases-comm-central"]["output_dir"]


def parse_advisory(url):
    """
    Scrapes given url and parses the advisory content
    :param url: string
        the advisory page url
    :returns: list
        list of cve_id, bug_id tuples
    """
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")
    sections = soup.findAll('section', {"class": "cve"})

    matches = []

    if len(sections) > 0:
        for sect in sections:
            cve_id = sect.find('h4')["id"]
            refs = [a["href"] for a in sect.find("ul").find("li").findAll(
                "a") if BUGZILLA_URL in a["href"] and "id=" in a["href"]]

            bug_ids = [a.split("id=")[1] for a in refs]
            bug_ids = [urllib.parse.unquote(
                id).split(',') if "%" in id else id for id in bug_ids]

            bug_ids = np.array(bug_ids).flatten()
            matches.extend([(cve_id, bug_id) for bug_id in bug_ids])
            print(cve_id, bug_ids)
    else:
        advisories = soup.findAll("div", {"class": "advisory"})
        for adv in advisories:
            uls = adv.findAll("ul")[1:]
            for ul_elem in uls:
                refs = []
                if len(ul_elem.findAll("li")) > 1:
                    for li_elem in ul_elem.findAll("li"):
                        refs.extend([a["href"] for a in li_elem.findAll("a")])
                else:
                    refs = [a["href"] for a in ul_elem.find("li").findAll("a")]

                bugs = [a for a in refs
                        if BUGZILLA_URL in a and "id=" in a]

                # pylint: disable = anomalous-backslash-in-string
                cve_id = [re.search('CVE-\d{4}-\d{4,7}', a)[0]
                          for a in refs if re.search('CVE-\d{4}-\d{4,7}', a)]
                # pylint: enable = anomalous-backslash-in-string

                if len(cve_id) > 0:
                    cve_id = cve_id[0]
                else:
                    continue

                bug_ids = [a.split("id=")[1].replace('\n', '')
                           for a in bugs]
                bug_ids = [urllib.parse.unquote(id).split(
                    ',') if "%" in id else id.split(',') for id in bug_ids]
                bug_ids = [b for bugs in bug_ids for b in bugs]

                matches.extend([(cve_id, bug_id)
                                for bug_id in bug_ids])
                print(cve_id, bug_ids)

    return matches


def run():
    """
    Runs the scraper end exports the results
    """
    response = requests.get(ADV_URL)

    soup = BeautifulSoup(response.text, "html.parser")

    urls = [BASE_URL + l.find('a')['href']
            for l in soup.findAll("li", {"class": "level-item"})
            if l.find('a') is not None]

    cve_matching = []
    for i, url in enumerate(urls):
        print(i, url)
        matches = parse_advisory(url)
        cve_matching.extend(matches)

    cve_matching_df = (pd.DataFrame(cve_matching,
                                    columns=["cve_id", "bug_id"])
                       .drop_duplicates())

    gecko_output_path = os.path.join(
        GECKO_OUTPUT, "gecko-dev_{}.csv".format(FILE_SUFFIX))
    releases_output_path = os.path.join(
        RELEASES_OUTPUT, "releases-comm-central_{}.csv".format(FILE_SUFFIX))

    cve_matching_df.to_csv(gecko_output_path, index=False)
    cve_matching_df.to_csv(releases_output_path, index=False)


if __name__ == "__main__":
    run()
