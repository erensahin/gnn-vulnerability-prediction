""" Scraper module for wireshark """

import os
import re
import requests
from bs4 import BeautifulSoup
import pandas as pd
from vulnerability_prediction import REPOS
from vulnerability_prediction.scrapers import FILE_SUFFIX

BASE_URL = 'https://www.wireshark.org/security/'
WS_OUTPUT = REPOS["wireshark"]["output_dir"]


def get_cve_regex():
    """
    Returns CVE regex pattern
    :returns: re.Pattern
        regex pattern
    """
    # pylint: disable = anomalous-backslash-in-string
    prog = re.compile("CVE-\d{4}-\d{4,7}", flags=re.IGNORECASE)
    # pylint: enable = anomalous-backslash-in-string

    return prog


def parse_advisory(url):
    """
    Scrapes given url and parses the advisory content
    :param url: string
        the advisory page url
    :returns: list
        list of cve_id, bug_id tuples
    """
    advisory_url = BASE_URL + url
    page = requests.get(advisory_url)
    soup = BeautifulSoup(page.content, 'html.parser')

    matches = []

    try:
        pattern = ' > '.join(['#advisory_background', 'div',
                              'div', 'div.col.col-md-8', 'div',
                              'p:nth-child(8)'])

        references = soup.select(pattern)[0]

        all_elems = references.findAll('a')
        bug_reports = [
            elem for elem in all_elems if "bugs.wireshark" in elem["href"]]

        prog = get_cve_regex()

        cve_ids = [prog.search(a["href"]).group()
                   for a in all_elems if prog.search(a["href"])]

        if len(bug_reports) > 1:
            for elem in bug_reports:
                bug_entry = match_from_bug_report(elem)
                if len(bug_entry) > 0:
                    print(advisory_url, bug_entry)
                    matches.extend(bug_entry)
                else:
                    print(advisory_url, elem["href"],
                          "has no see also section")
        elif len(bug_reports) == 1:
            elem = bug_reports[0]
            bug_entry = match_from_bug_report(elem)
            if len(bug_entry) > 0:
                print(advisory_url, bug_entry)
                matches.extend(bug_entry)
            else:
                print("** Other branch for", advisory_url)
                bug_id = elem["href"].split("id=")[1]
                print(advisory_url, elem["href"], "Matched to", cve_ids)

                for cve_id in cve_ids:
                    matches.append({
                        "bug_date": "",
                        "bug_id": bug_id,
                        "cve_id": cve_id
                    })

        elif len(bug_reports) > 1 and len(cve_ids) == 1:
            print("*** Third branch for", advisory_url)
            for elem in bug_reports:
                bug_entry = match_from_bug_report(elem)
                if len(bug_entry) > 0:
                    print(advisory_url, bug_entry)
                    matches.extend(bug_entry)
                else:
                    bug_id = elem["href"].split("id=")[1]
                    print(advisory_url, elem["href"], "Matched to", cve_ids[0])
                    matches.append({
                        "bug_date": "",
                        "bug_id": bug_id,
                        "cve_id": cve_ids[0]
                    })
        else:
            print("--- {} did not match any branch".format(advisory_url))
    except IndexError as ex:
        print("Ex: ", advisory_url, str(ex))

    return matches


def match_from_bug_report(elem):
    """
    Given an html element, match bug report regex
    :param elem: string
        the advisory page url
    :returns: list of bug entries
    """
    bug_id = elem["href"].split("id=")[1]

    bug_page = requests.get(elem["href"])
    bug_soup = BeautifulSoup(bug_page.content, 'html.parser')

    pattern = '#bz_show_bug_column_2 > table > tr:nth-child(1)'
    bug_date = bug_soup.select(pattern)[0].td.contents[0].split(' ')[0]

    see_also = bug_soup.select('#field_container_see_also > ul > li > a')

    prog = get_cve_regex()
    cve_ids = [prog.search(s["href"]).group()
               for s in see_also if prog.search(s["href"])]

    bug_entrys = []
    for cve_id in cve_ids:
        bug_entrys.append({
            "bug_id": bug_id,
            "cve_id": cve_id,
            "bug_date": bug_date
        })

    return bug_entrys


def run():
    """ Runs the scraper """
    page = requests.get(BASE_URL)
    soup = BeautifulSoup(page.content, 'html.parser')
    advisories = [x.a["href"] for x in soup.select('.security-advisory-entry')]
    cve_matching = []

    for url in advisories:
        matches = parse_advisory(url)
        cve_matching.extend(matches)

    cve_matching_df = pd.DataFrame(cve_matching)
    output_path = os.path.join(
        WS_OUTPUT, "wireshark_{}.csv".format(FILE_SUFFIX))
    cve_matching_df.drop_duplicates().to_csv(output_path, index=False)


if __name__ == "__main__":
    run()
